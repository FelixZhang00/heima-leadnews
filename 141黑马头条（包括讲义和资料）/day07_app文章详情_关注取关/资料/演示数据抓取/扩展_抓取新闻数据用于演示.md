```
163新闻网址
https://3g.163.com/touch/news/sub/history/?ver=c&clickfrom=index2018_header_main

https://3g.163.com/touch/ent/?ver=c&clickfrom=index2018_header_main
```

**获取数据实现**



**jsoup介绍**

   jsoup 是一款Java 的HTML解析器，可直接解析某个URL地址、HTML文本内容。它提供了一套非常省力的API，

　可通过DOM，CSS以及类似于jQuery的操作方法来取出和操作数据。

Document 对象       javascript   DOM   BOM

  1）从一个URL，文件或字符串中解析HTML

  2）使用DOM或CSS选择器来查找、取出数据

  3）可操作HTML元素、属性、文本

  注意：jsoup是基于MIT协议发布的，可放心使用于商业项目。

**jsoup官网:** https://jsoup.org/



**webmagic官网:** http://webmagic.io/docs/zh/

需要依赖

```xml
        <dependency>
            <groupId>org.jsoup</groupId>
            <artifactId>jsoup</artifactId>
            <version>1.13.1</version>
        </dependency>
		<dependency>
            <groupId>org.seleniumhq.selenium</groupId>
            <artifactId>selenium-java</artifactId>
            <version>3.5.1</version>
        </dependency>
```



代码实现

```java
package com.heima.wemedia;
import java.io.*;
import java.net.URLDecoder;
import java.util.*;
import com.alibaba.fastjson.JSON;
import com.baomidou.mybatisplus.core.toolkit.Wrappers;
import com.heima.file.service.FileStorageService;
import com.heima.model.threadlocal.WmThreadLocalUtils;
import com.heima.model.wemedia.dtos.WmNewsDTO;
import com.heima.model.wemedia.pojos.WmMaterial;
import com.heima.model.wemedia.pojos.WmNews;
import com.heima.model.wemedia.pojos.WmUser;
import com.heima.wemedia.mapper.WmMaterialMapper;
import com.heima.wemedia.service.WmNewsService;
import org.apache.commons.lang3.StringUtils;
import org.apache.tomcat.util.http.fileupload.IOUtils;
import org.jsoup.Jsoup;
import org.jsoup.nodes.Document;
import org.jsoup.nodes.Element;
import org.jsoup.select.Elements;
import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.Test;
import org.openqa.selenium.WebDriver;
import org.openqa.selenium.chrome.ChromeDriver;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.boot.test.context.SpringBootTest;
import java.net.HttpURLConnection;
import java.net.URL;
/**
 * 抓取新闻数据  用于app端演示
 **/
@SpringBootTest
public class ReptilesArticleData {
    @Autowired
    WmNewsService wmNewsService;
    @Autowired
    WmMaterialMapper wmMaterialMapper;
    @Autowired
    FileStorageService fileStorageService;
    @Value("${file.oss.web-site}")
    String webSite;
    /**
     * 直接通过调用修改状态方法，将所有爬虫数据 设置通过
     */
    @Test
    public void authPassWmNews(){
        List<WmNews> list = wmNewsService.list(Wrappers.<WmNews>lambdaQuery().eq(WmNews::getStatus, 3));
        for (WmNews wmNews : list) {
            NewsAuthDTO dto = new NewsAuthDTO();
            dto.setId(wmNews.getId());
            wmNewsService.updateStatus((short)4,dto);
        }
    }
    /**
     * 直接通过调用修改状态方法，将所有爬虫数据 设置通过
     */
    Document document = null;
    @BeforeEach
    public void initDriver(){
        System.setProperty("webdriver.chrome.driver", "C:\\worksoft\\tools\\chromedriver.exe");
        WebDriver driver = new ChromeDriver();
        driver.get("https://3g.163.com/touch/news/sub/history/?ver=c&clickfrom=index2018_header_main");
        document = Jsoup.parse(driver.getPageSource());
    }

    /**
     * 获取数据  并封装成wmNews
     * @throws IOException
     */
    @Test
    public void reptilesData() throws IOException {
        // 模拟当前自媒体登录用户
        WmUser wmUser = new WmUser();
        wmUser.setId(1102);
        WmThreadLocalUtils.setUser(wmUser);
        // 获取网易新闻数据
        // 娱乐频道:https://3g.163.com/touch/ent/?ver=c&clickfrom=index2018_header_main
//        String url = "https://3g.163.com/touch/ent/?ver=c&clickfrom=index2018_header_main";
        // 获取该网页document文档数据
//        Document document = Jsoup.connect(url)
//                .header("User-Agent", "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.183 Safari/537.36")
//                .get();
        // 找到指定class的所有元素
        Elements elementsByClass = document.getElementsByClass("tab-content");
        System.out.println("div元素个数:  " + elementsByClass.size());
        Element element = elementsByClass.get(0);
        // 找到所有article标签页面元素
        Elements articleList = element.getElementsByTag("article");
        for (Element article : articleList) {

            String href = null;
            String title = null;
            Elements imgList = null;
            try {
                Element aElement = article.getElementsByTag("a").get(0);
                href = aElement.attr("href");
                System.out.println("新闻的url路径: "+href);
                Element titleEle = aElement.getElementsByClass("title").get(0);
                title = titleEle.text();
                System.out.println("新闻的文章标题:"+title);
                Element newsPic = aElement.getElementsByClass("news-pic").get(0);
                // 获取封面图片元素集合
                imgList = newsPic.getElementsByTag("img");
            } catch (Exception e) {
                e.printStackTrace();
                continue;
            }
            // 封装WmNewsDto对象
            WmNewsDTO wmNewsDto = new WmNewsDTO();
            // 解析单个文章详情
            List<Map> contentMap = parseContent(href);
            wmNewsDto.setContent(JSON.toJSONString(contentMap));
            // 封面图片集合
            List<String> urlList = new ArrayList<>();
            for (Element imgEle : imgList) {
                String src = imgEle.attr("data-src");
                System.out.println("封面图片url==>"+src);
                String fileName = uploadPic(src);
                if(StringUtils.isNotBlank(fileName)){
                    // 如果上传图片路径不为空  创建素材信息
                    WmMaterial wmMaterial = new WmMaterial();
                    wmMaterial.setUserId(WmThreadLocalUtils.getUser().getId());
                    wmMaterial.setUrl(fileName);
                    wmMaterial.setType((short)0);
                    wmMaterial.setIsCollection((short)0);
                    wmMaterial.setCreatedTime(new Date());
                    wmMaterialMapper.insert(wmMaterial);
                    urlList.add(fileName);
                }
            }
            wmNewsDto.setTitle(title);
            wmNewsDto.setType((short) urlList.size());
            if(urlList.size()>0){
                wmNewsDto.setImages(urlList);
            }
            wmNewsDto.setChannelId(6);
            try {
                Thread.sleep(1000); // 睡眠1秒 让发布时间不一致
            } catch (InterruptedException e) {
                e.printStackTrace();
            }
            wmNewsDto.setPublishTime(new Date());
            wmNewsDto.setStatus((short)3); // 待审核状态
            wmNewsDto.setLabels("爬虫");
            wmNewsService.submitNews(wmNewsDto);
        }
    }
    /**
     * 封面图片url==>//nimg.ws.126.net/?url=http%3A%2F%2Fdingyue.ws.126.net%2F2020%2F0227%2Fd5ba8ca7j00q6cggh000vc000hs00b7m.jpg&thumbnail=234x146&quality=85&type=jpg
     * @param imgUrl
     * @return
     */
    public String uploadPic(String imgUrl){
        String imageUrl = getImageUrl(imgUrl);
        InputStream in = getInputStreamByUrl(imageUrl);
        String uuid = UUID.randomUUID().toString().replace("-","");
        String suffix = imageUrl.substring(imageUrl.lastIndexOf("."));
        if(in!=null){
            String fileName = fileStorageService.store("material", uuid+suffix, in);
            System.out.println("上传文件名称: "+fileName);
            return fileName;
        }
        return null;
    }
    public String getImageUrl(String url)  {
        try {
//            String url = "//nimg.ws.126.net/?url=http%3A%2F%2Fdingyue.ws.126.net%2F2020%2F0227%2Fd5ba8ca7j00q6cggh000vc000hs00b7m.jpg&thumbnail=234x146&quality=85&type=jpg";
            url = url.split("&")[0];
            url = url.substring(url.indexOf("url=")).replaceAll("url=","");
            url = URLDecoder.decode(url, "utf8");
            return url;
        } catch (Exception e) {
            e.printStackTrace();
            throw new RuntimeException("解析图片路径出现异常");
        }
    }
    /**
     * 工具方法:  根据url路径 获取输入流
     * @param strUrl
     * @return
     */
    public static InputStream getInputStreamByUrl(String strUrl){
        if(!strUrl.startsWith("http")){
            strUrl = "http:" + strUrl;
        }
        HttpURLConnection conn = null;
        try {
            URL url = new URL(strUrl);
            conn = (HttpURLConnection)url.openConnection();
            conn.setRequestMethod("GET");
            conn.setConnectTimeout(20 * 1000);
            final ByteArrayOutputStream output = new ByteArrayOutputStream();
            IOUtils.copy(conn.getInputStream(),output);
            return  new ByteArrayInputStream(output.toByteArray());
        } catch (Exception e) {
            e.printStackTrace();
        }finally {
            try{
                if (conn != null) {
                    conn.disconnect();
                }
            }catch (Exception e){
                e.printStackTrace();
            }
        }
        return null;
    }

    /**
     * 解析文章详情内容
     * @param href
     * @return
     */
    private List<Map> parseContent(String href)  {
        String url="http:"+href;
        List<Map> contentMap = new ArrayList<>();
        Document document = null;
        try {
            document = Jsoup.connect(url)
                    .header("User-Agent", "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.183 Safari/537.36")
                    .get();
                Element contentDiv = document.getElementsByClass("content").get(0);
                Element pageDiv = contentDiv.getElementsByTag("div").get(0);
                Elements allElements = pageDiv.getAllElements();
                for (Element subElement : allElements) {
                    String tagName = subElement.tagName();
                    String className = subElement.className();
                    if("p".equalsIgnoreCase(tagName)){
                        Map map = new HashMap();
                        map.put("type","text");
                        map.put("value",subElement.text());
                        contentMap.add(map);
                        System.out.println("文本内容: " + subElement.text());
                    }
                    if("div".equalsIgnoreCase(tagName)&&"photo".equalsIgnoreCase(className)){
                        System.out.println("图片内容: " + subElement.text());
                    }
                }
        } catch (Exception e) {
            e.printStackTrace();
            Map map = new HashMap();
            map.put("type","text");
            map.put("value","测试文章内容");
            contentMap.add(map);
        }
        return contentMap;
    }
}
```



目前网易以采用延迟加载图片方式,  所以直接获取路径连接无法得到图片属性

可以安装谷歌浏览器驱动 ，相当于真实的通过浏览器打开指定url, 待所有延迟信息加载完毕后 通过该驱动获取页面html

在通过jsoup 将html转为Document对象进行解析

































**禁区1：为违法违规组织提供爬虫相关服务（验证码识别服务贩卖、SEO……）**

**禁区2：个人隐私数据抓取与贩卖**

**禁区3：利用无版权的商业数据获利**

 

**非法获取计算机系统数据罪**

> 根据《中华人民共和国刑法》第二百八十五条规定，非法获取计算机信息系统数据、非法控制计算机信息系统罪，是指违反国家规定，**侵入国家事务、国防建设、尖端科学技术领域以外的计算机信息系统或者采用其他技术手段，获取该计算机信息系统中存储、处理或者传输的数据**，情节严重的行为。刑法第285条第2款明确规定，犯本罪的，**处三年以下有期徒刑或者拘役，并处或者单处罚金；情节特别严重的，处三年以上七年以下有期徒刑，并处罚金。**

**侵犯商业秘密罪**

> 《反不正当竞争法》第九条，以不正当手段获取他人商业秘密的行为即已经构成侵犯商业秘密。而后续如果进一步利用，或者公开该等信息，则构成对他人商业秘密的披露和使用，同样构成对权利人的商业秘密的侵犯。

**非法侵入计算机信息系统罪**

> 《刑法》第二百八十六条还规定，违反国家规定，对计算机信息系统功能进行删除、修改、增加、干扰，造成计算机信息系统不能正常运行，后果严重的，构成犯罪，处五年以下有期徒刑或者拘役；后果特别严重的，处五年以上有期徒刑。而违反国家规定，对计算机信息系统中存储、处理或者传输的数据和应用程序进行删除、修改、增加的操作，后果严重的，也构成犯罪，依照前款的规定处罚。

**网络安全法**

> 《网络安全法》第四十四条 任何个人和组织不得窃取或者以其他非法方式获取个人信息。因此，如果爬虫在未经用户同意的情况下大量抓取用户的个人信息，则有可能构成非法收集个人信息的违法行为。

**民法总则**

> 《民法总则》第111条任何组织和个人需要获取他人个人信息的，应当依法取得并确保信息安全。不得非法收集、使用、加工、传输他人个人信息

**侵犯公民个人信息罪**

> 《刑法》修正案（九）中将刑法第二百五十三条进行了修订，明确规定违反国家有关规定，向他人出售或者提供公民个人信息，情节严重的，构成犯罪；在未经用户许可的情况下，非法获取用户的个人信息，情节严重的也将构成“侵犯公民个人信息罪”。根据《最高人民法院 最高人民检察院关于办理侵犯公民个人信息刑事案件适用法律若干问题的解释》第五条规定，对“情节严重”的解释，（1）非法获取、出售或者提供行踪轨迹信息、通信内容、征信信息、财产信息五十条以上的；（2）非法获取、出售或者提供住宿信息、通信记录、健康生理信息、交易信息等其他可能影响人身、财产安全的公民个人信息五百条以上的；（3）非法获取、出售或者提供第三项、第四项规定以外的公民个人信息五千条以上的便构成“侵犯公民个人信息罪”所要求的“情节严重”。

